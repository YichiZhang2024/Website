{
  "articles": [
    {
      "path": "about.html",
      "title": "Yichi Zhang 张亦弛",
      "description": "A little bit about myself ",
      "author": [],
      "contents": "\n\n\nHi, I am a quantitative researcher at the American Institutes for Research (AIR), where I apply advanced statistical techniques to support the design, analysis, and evaluation of large-scale educational assessments. My work focuses on the validity, reliability, and fairness of psychological and behavioral measurement through applied statistical modeling and psychometric analysis.\nI earned my Ph.D. in Quantitative Methods and Computational Psychology from the University of Southern California (USC), where I was co-advised by Dr. Hok Chio(Mark) Lai and Dr. Rand Wilcox. My training emphasized robust statistical methods, measurement theory, and computational approaches to behavioral science.\nI first discovered my passion for computational social science as an undergraduate at Dickinson College, where I double majored in Mathematics and Psychology. Since then, I’ve become increasingly interested in using statistical modeling and machine learning to understand human behavior and drive informed decision-making in both research and applied contexts.\n\n\n\n\n\nI’m passionate about bridging the gap between rigorous research and real-world impact through academic collaborations, user experience research, or data science applications. Check out the Research page for specific projects and Publication page for my publications. CV can be found here.\n\n\n\n",
      "last_modified": "2025-05-07T16:46:56-07:00"
    },
    {
      "path": "blog.html",
      "title": "Multilevel Models in Julia ",
      "author": [
        {
          "name": "Yichi Zhang",
          "url": {}
        }
      ],
      "date": "2021-12-23",
      "contents": "\nToday We will have a fun session doing regression and multilevel models(MLMs) using Julia. First, we will have a small\nexercise fitting a linear regression model to a sample dataset, and then we will fit the MLM to the same dataset and\ncompare the difference in results. Please note the examples come from Dr.Mark Lai’s multilevel modeling course, which\ncan be assessed at https://quantscience.rbind.io/courses/psyc575/homework-3/. Can’t wait? Let’s get started!\nSet up\nInstall Packages\nIn Julia, we can add package by typing “]” in the Terminal to enter the Pkg mode and exit using Ctrl + C. Or, we\ncan use [Pkg] package and add multiple packages at once.\n#= uncomment below to install packages\nusing Pkg\nPkg.add([\"Pipe\", \"DataFrames\", \"StatFiles\", \"GLM\",\"MixedModels\",\"Plots\", \"StatsPlots\"])\n=#\nUsing Packages\nJust like we use library to load packages in R, we use using in Julia. It’s better to have a separate session\nin the beginning of the file to load all packages you will need in the analysis. Otherwise it will take a long time\nto precompile each time.\nPipe for pipe operator.\nDataFrames for working with dataframes.\nStatFiles for reading datasets from Stata, SPSS and SAS. If you have a txt file, can try CSV.read function from package CSV.\nGLM for linear regression.\nMixedModels for working with multilevel models. Similar to lme4.\nPlots for making graphs.\nStatsPlots for making graphs.\nusing Pipe, DataFrames, StatFiles, GLM, MixedModels, Plots, StatsPlots, Statistics\nDataset\nToday we are going to use the dataset from the World Value Survey-1990-93 data (World Values Study Group, 1994). There\nare five variables in the data set:\nCountryID: CountryIDcountry: Country’s namegm_GNP: Grand-mean centered Gross National Productincome: Income level(0-least income to 9-most income)happy:Feel happy(1-not happy to 4-very happy)\nThis data set and set of practice problem come from Mark’s Multilvel Modeling class, so thanks Mark!\ndata_happy = DataFrame(load(\"happy_combined.sav\"))\n#= Or we can use pipe operator\ndata_happy = load(\"happy_combined.sav\") |> DataFrame\n=#\ndata_happy = dropmissing(data_happy)\nResearch Questions\nAre people with higher individual level income happier? Is the relation similar across countries? How is the result\nof linear regression different from the result of multilevel models?\nDescriptive Analysis\n## Summary of all variables in the dataset\ndescribe(data_happy)\n## list first five rows of data\nfirst(data_happy,5)\n## list names of all variables\nnames(data_happy)\n## get size of the data set\nsize(data_happy)\nsize(data_happy,1)\nsize(data_happy,2)\n## check a specific column \ndata_happy[:,\"country\"]\nunique(data_happy[:,\"country\"])\n# data_happy[!,:2]; data_happy[:,2] also work for extracting the second columns\n# data_happy[2,:] can extract the second row, data_happy[2:5,:] extract the second to fifth row.\n@df data_happy scatter(\n    :country, \n    :happy,\n    group = :country)\nLinear Regression\n???Exercise Time: We are familiar with our dataset, so let’s do some exercise! Recall Winnie did a great presentation last time on linear\nregression, so please go ahead to fit a linear regression model and write out the equation.\nHint: Model_name = lm(@formula(DV ~ IV), data_set)\nFitting the linear regression model\nlm1 = lm(@formula(happy~ gm_GNP),data_happy)\n# extract coefficients\ncoef(lm1)\n# extract standard errors\nstderror(lm1)\n# extract variance covariance matrix\nvcov(lm1)\n# obtain R^2\nr2(lm1)\n# get the deviance \ndeviance(lm1)\nThis model explains 4.72% of variance in happy. Note the standard error estimates for gm_GNP is 0.010, t =17.13,\n95% CI [0.155 0.195], p < 0.0001.\nEquations\n\\text{happy} = 2.992 + 0.175 \\text{gm_GNP}\nMultilevel Modeling\nRandom Intercept Model\nWe first fit a random intercept model and calculate the intraclass correlation. Recall\nEquations\nLevel 1:\n\\text{happy}_{ij} = \\beta_{0j} + e_{ij}\nLevel 2:\n\\beta_{0j} = \\gamma_{00} + u_{0j}\n\\text{ICC} = \\frac{\\tau_0^2}{\\tau_0^2 + \\sigma^2}\n## Fitting MLMs\nmm1 = fit(LinearMixedModel, @formula(happy ~ (1|country)), data_happy)\n## Create a vector and store the model fit statistics\nmodel_fit= Vector{Float64}()\npush!(model_fit,aic(mm1))\n??? Exercise Time: What is the value of ICC?\nThe first part of the result prints out estimation method and the model fit statistics, such as AIC,BIC, etc. The second\npart is the table of estimates of parameters associated with the random effects. The third part is the fixed effects\npoint estimates and standard errors.\nICC = 0.0649/(0.0649 + 0.4842) = 0.118, so there is evidence that people’s happiness level varies across countries.\nVariability at the country level accounts for 11.8% of the total variability of happiness level.\nDesign effect = 1 +(average cluster size - 1) x ICC. We have 5926 observations and 38 groups, so\ndesign effect = 1 + (5926/38 -1) x 0.118 = 19.28.\nAdd Level 2 Predictor\nIt is reasonable to think gm_GNP is a cluster level predictor, so let’s add it to our model.\nEquations\nLevel 1:\n\\text{happy}_{ij} = \\beta_{0j} + e_{ij}\nLevel 2:\n\\beta_{0j} = \\gamma_{00} + \\gamma_{01} \\text{gm_GNP}_{j} + u_{0j}\\\\\n## Fitting MLMs\nmm2 = fit(LinearMixedModel, @formula(happy ~ gm_GNP + (1|country)), data_happy)\npush!(model_fit, aic(mm2))\n# extract log likelihood\nloglikelihood(mm2)\n# extract Akaike's Information Criterion\naic(mm2)\n# extract Bayesian Information Criterion\nbic(mm2)\n# extract degrees of freedom\ndof(mm2)\n# extract coefficient\ncoef(mm2)\n# extract fixed effects\nfixef(mm2)\nvcov(mm2)\nstderror(mm2)\n# extract coefficients table\ncoeftable(mm2)\n# extract variance components\nVarCorr(mm2)\n# return sigma^2\nvarest(mm2)\n# return tau\nVarCorr(mm2).σρ[1][1][1]\n# return elements in the components\ndump(VarCorr(mm1))\n# return sigma\nsdest(mm2)\n# extract random effects\nranef(mm2)\nNote that the result is slightly different from the R output, it’s because the default estimation method in\n[MixedModels] in Julia is maximum likelihood estimation, whereas the default estimation method for [lme4] in R is\nRestricted maximum likelihood method.\nThis time standard error estimates for gm_GNP is 0.0337, z = 5.33, p < 0.0001. The SE for OLS is smaller than\nthe SE for MLM, indicating OLS underestimates the SE.\nAdd Level 1 predictor\nBecause relationships at one level are not necessarily the same at the other level, we need to be careful adding\nthe level 1 predictor. Two approaches to decompose the impact of level 1 predictor on outcome variables:\ncluter mean centering + cluster mean and the raw predictor + clutster mean. Here we will use the first approach.\nEquations\nLevel 1:\n\\text{happy}_{ij} = \\beta_{0j} + \\beta_{1j} \\text{income_cmc}_{ij} + e_{ij}\nLevel 2:\n\\beta_{0j} = \\gamma_{00} + \\gamma_{01} \\text{income_mean}_{j} + u_{0j}\\\\\n\\beta_{1j} = \\gamma_{10} + u_{1j} \n## Cluster mean centering\ndata_happy2 = @pipe data_happy |> \n            groupby(_,:country) |> # group by country\n            transform(_, :income => mean => :income_mean)|> # add a new variable income_mean\n            transform(_, [:income, :income_mean]=> ByRow(-) => :income_cmc) # add a new variable the centered variable\n\n## Fitting MLMs\nmm3 = fit(LinearMixedModel, @formula(happy ~ income_cmc + income_mean + (income_cmc|country)), data_happy2)\npush!(model_fit, aic(mm3))\nBoth income_cmc and income are significant predictors of happiness level. For a person from a country with\nincome_mean = 0 and this person has average country level income, the predicted happiness level is 2.66, SE = 0.16.\nThe average within country slope is 0.047(SE = 0.008), meaning a one unit increase in income_cmc is associated with a\n0.047 unit increase in happiness. This slope varies across countries, with a standard deviation of 0.38. The average between\ncountry level slope is 0.08, SE = 0.04, suggesting a one unit increase in income_mean is associated with a 0.08 unit\nincrease in the average happiness level.\nCross level Interaction\nIs the relation between happy and income moderated by gm_GNP? We can answer this question by adding gm_GNP to\nthe above model.\nmm4 = fit(LinearMixedModel, @formula(happy ~ income_cmc * gm_GNP + income_mean + (income_cmc|country)), data_happy2)\npush!(model_fit, aic(mm4))\nAfter adding gm_GNP, the impact of income_mean on happy is not significant.\nEquations\nLevel 1:\n\\text{happy}_{ij} = \\beta_{0j} + \\beta_{1j} \\text{income_cmc}_{ij} + e_{ij}\nLevel 2:\n\\beta_{0j} = \\gamma_{00} + \\gamma_{01} \\text{income_mean}_{j} + \\gamma_{02} \\text{gm_GNP}_{j} + u_{0j}\\\\\n\\beta_{1j} = \\gamma_{10} + \\gamma_{11} \\text{gm_GNP}_{j} + u_{1j} \n??? Exercise time: 1. Which model fits data better?\n# print out the AIC\nprint(model_fit)\n# Likelihood Ratio Test\nMixedModels.likelihoodratiotest(mm1,mm2,mm3,mm4)\nConclusion\nThe results of [MixedModels] in Julia and [lme4] are slightly different due to the estimation methods that are used.\nThe cross-level interaction model fits best to our data, suggesting the country level GNP gm_GNP moderated the relation\nbetween individual’s happiness level happy and income income.\nResources\nMark’s MLM class and HW\nDataFrames package\nhttps://dataframes.juliadata.org/v0.14.0/index.html\nMixedModels Package (similar to lme4 in R) documentation\nhttps://juliastats.org/MixedModels.jl/v1.0/index.html\nMaybe useful for plotting week:\nGadfly Package (similar to ggplot in R) documentation\nhttps://juliastats.org/MixedModels.jl/v1.0/index.html\n\n\n\n",
      "last_modified": "2025-05-07T16:46:57-07:00"
    },
    {
      "path": "index.html",
      "title": "Blog",
      "description": "Welcome to the website. I hope you enjoy it!\n",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2025-05-07T16:46:57-07:00"
    },
    {
      "path": "publications.html",
      "title": "Publications",
      "description": "My recent publications and conference presentations. ",
      "author": [],
      "contents": "\nPeer-Reviewed Articles\n2025\n\nLai, M. H. C., Y. Zhang, M. Ozcan, et al. (2025). “f MACS  :\nGeneralizing d MACS Effect Size for Measurement Noninvariance\nwith Multiple Groups and Multiple Grouping Variables”. In:\nStructural Equation Modeling: A Multidisciplinary Journal,\np. 1–9. ISSN: 1532-8007. DOI:\n10.1080/10705511.2025.2484812.\n\n2024\n\nLai, M. H. C., Y. Zhang, and F. Ji (2024). “Correcting for\nSampling Error in between-Cluster Effects: An Empirical Bayes\nCluster-Mean Approach with Finite Population Corrections”. In:\nMultivariate Behavioral Research 59.3, p. 584–598. ISSN:\n1532-7906. DOI:\n10.1080/00273171.2024.2307034.\nZhang, Y. and M. H. C. Lai (2024a). “Bayesian Region of\nMeasurement Equivalence (ROME) approach with alignment”.\n(Under Review).\nZhang, Y. and M. H. C. Lai (2024b). “Evaluating two\nsmall-sample corrections for fixed-effects standard errors and\ninferences in multilevel models with heteroscedastic,\nunbalanced, clustered data”. In: Behavior Research Methods\n56.6, p. 5930–5946. ISSN: 1554-3528. DOI:\n10.3758/s13428-023-02325-9.\nZhang, Y., W. W. Tse, and M. H. C. Lai (2024). “Bootstrap\nmethods for multilevel data when asymptotic distributional\nassumptions are not tenable”. Dependent Data in Social\nSciences Research: Forms, Issues, and Methods of Analysis.\n\n2023\n\nOzturk, E. D., Y. Zhang, M. H. C. Lai, et al. (2023).\n“Measurement Invariance of the Neurobehavioral Symptom\nInventory in Male and Female Million Veteran Program Enrollees\nCompleting the Comprehensive Traumatic Brain Injury\nEvaluation”. In: Assessment 31.5, p. 967–979. ISSN:\n1552-3489. DOI:\n10.1177/10731911231198214.\nTse, W. W., M. H. C. Lai, and Y. Zhang (2023). “Does strict\ninvariance matter? Valid group mean comparisons with\nordered-categorical items”. In: Behavior Research Methods\n56.4, p. 3117–3139. ISSN: 1554-3528. DOI:\n10.3758/s13428-023-02247-6.\nZhang, Y., M. H. C. Lai, and G. J. Palardy (2023). “A Bayesian\nregion of measurement equivalence (ROME) approach for\nestablishing measurement invariance.” In: Psychological\nMethods 28.4, p. 993–1004. ISSN: 1082-989X. DOI:\n10.1037/met0000455.\n\n2022\n\nLai, M. H. C. and Y. Zhang (2022). “Classification Accuracy of\nMultidimensional Tests: Quantifying the Impact of\nNoninvariance”. In: Structural Equation Modeling: A\nMultidisciplinary Journal 29.4, p. 620–629. ISSN: 1532-8007.\nDOI:\n10.1080/10705511.2021.1977936.\n\nConference Presentations\n2025\nZhang, Y., & Lai, M. H. C. (2025, April 23-26). Bayesian Region of Measurement Equivalence (ROME) Framework With Multilevel Confirmatory Factor Analysis [Poster Session]. The National Council on Measurement in Education (NCME), Dever, Colorado.\n2024\nZhang, Y., & Wolfe, E. W. (2024, April 11-14). Evaluating subgroup analysis indices and guidelines for automated scoring algorithm [Poster session]. The National Council on Measurement in Education (NCME), Philadelphia, Pennsylvania.\n2023\nZhang, Y. & Lai, M. H. C. (2023, July 25-28). Alignment with Bayesian Region of Measurement Equivalence (ABROME) Approach for Multiple Groups Comparisons [Oral Presentation]. Annual Meeting of the Psychometric Society (IMPS), College Park, Maryland.\nZhang, Y., Kim, Y., & Zheng, X. (2023, April 12-15). Investigating Measurement Invariance in NAEP Student Questionnaire Index Items [Oral Presentation]. The National Council on Measurement in Education (NCME), Chicago, Illinois.\nLai, M. H. C., Zhang, Y., & Feng J.(2023, April 13-16). An empirical Bayes cluster-mean approach to correct for sampling error in between-cluster effects [Poster session]. American Educational Research Association Annual Meeting (AERA), Chicago, Illinois.\n2022\nZhang, Y. & Lai, M. H. C. (2022, July 11-15). Bayesian Region of Measurement Equivalence Approach with Alignment [Oral Presentation]. Annual Meeting of the Psychometric Society (IMPS), Bologna, Italy.\nZhang, Y. & Lai, M. H. C. (2022, July 11-15). Inferences with Multilevel Model: What if You have Small, Unbalanced, Heteroscedastic Samples? [Poster Presentation]. Annual Meeting of the Psychometric Society (IMPS), Bologna, Italy.\nZhang, Y. & Lai, M. H. C. (2022, April 21-26). Evaluating Standard Error Estimators for Multilevel Models on Small Samples With Heteroscedasticity and Unbalanced Cluster Sizes. American Educational Research Association Annual Meeting, San Diego, Ca, United States.\n2021\nZhang, Y. & Lai, M. H. C. (2021, August 12-14). Classification accuracy of multidimensional tests: Quantifying the impact of noninvariance [Poster Session]. American Psychological Association Annual Convention (APA), Online.\nZhang, Y. & Lai, M. H. C. (2021, July 20-23). Classification accuracy of multidimensional tests: Quantifying the impact of noninvariance [Oral Presentation]. Annual Meeting of the Psychometric Society (IMPS), College Park, MD, United States.\nZhang, R., Zhang, Y., & Lalonde, R. (2021, July 27-31). Examining multiculturalism-creativity link from the perspective of challenge and threat appraisals [Oral Presentation]. International Association of Cross-Cultural Psychology (IACCP), online.\n2020\nZhang, Y. & Lai, M. H. C. (2020, July 14-17). A Bayesian Region of Measurement Equivalence (ROME) Approach for Establishing Measurement Invariance [Poster Session]. Annual Meeting of the Psychometric Society (IMPS), College Park, MD, United States. Schedule\nZhang, Y. & Lai, M. H. C. (2020, June 2-3). A Bayesian Region of Measurement Equivalence (ROME) Approach for Establishing Measurement Invariance [Poster Session]. Modern Modeling Methods Conference (MMM), Storrs, CT, United States. (Conference canceled)\n\n\n\n",
      "last_modified": "2025-05-07T16:47:00-07:00"
    },
    {
      "path": "research.html",
      "title": "Research",
      "author": [],
      "contents": "\nMy research interests include:\nMeasurement bias and fairness in surveys and assessments\nStatistical methods for robust, reproducible research\nApplied machine learning in social and behavioral sciences\nQuantitative methods in UX research, education, and healthcare\nCurrent Projects\nBayesian Region of Measurement Equivalence (ROME) Approach for Establishing Measurement Invariance\nPsychological scales are widely used when making decisions in personnel selections and college admissions. However, as the current generation has become more racially and ethnically diverse, people might react differently to scale items due to their diverse backgrounds and experiences. Thus, some scale items may contain systematic bias that leads to a higher score for some subgroups. While there is abundant research focusing on detecting such bias, most of them rely on the Null hypothesis testing (NHST) framework, with little attention to the degree of bias and the practical impact of bias.\nI proposed a Bayesian region of measurement equivalence (ROME) method for establishing measurement invariance, which allows researchers to quantify the degree of item bias on total scale scores and decide whether the group difference caused by biased indicators is negligible. This method allows researchers to interpret the noninvariance from a practical decision-making perspective. I applied this method on the National Assessment of Educational Progress data during my internship at the American Institutes for Research in 2022.\nEvaluating Standard Error Estimators for Multilevel Models on Small Samples With Heteroscedasticity and Unbalanced Cluster Sizes\nMultilevel modeling (MLM) is commonly used in psychological research to model clustered data. However, data in applied research usually come from small samples and have heteroscedastic variances and unbalanced cluster sizes, which violates one of the essential assumptions of MLM - homogeneity of variance. While the fixed-effect estimates produced by the maximum likelihood method remain unbiased, the standard errors for the fixed effects are mis-estimated, resulting in inaccurate inferences and inflated or deflated Type I error rates. Small-sample corrections, such as the Kenward-Roger (KR) adjustment and the adjusted cluster-robust standard errors (CR-SEs), have been proposed in literature.\nMy research compares KR with random slope (RS) models and the adjusted CR-SEs with Ordinary Least Squares (OLS), random intercept (RI) and RS models to analyze small, heteroscedastic, clustered data using a Monte Carlo simulation. I illustrated the use of these two small sample corrections on empirical data and provide guidelines for applied researchers.\nEvaluating Subgroup Analysis Indices and Guidelines for Automated Scoring Algorithm\nAutomated Essay Scoring (AES) is an application of artificial intelligence that predicts the scores humans would assign to essays based on the content and features of those essays. Williamson, Xi, and Breyer (2012) offered guidelines for evaluating an AES system, and those guidelines recommend that stakeholders evaluate fairness by examining scores assigned to demographic subgroups with respect to distribution shifts and levels of agreement between AES and human scores.\nIn my research, I evaluated the performance of AES agreement indices used for subgroup analyses using a Monte Carlo simulation, in which I varied sample size, distribution shape, and the number of score points when data are generated to produce predetermined levels of agreement or mean score offset between human scores and AES. This project was presented at the 2024 National Council on Measurement in Education (NCME) Annual Meeting.\n\n\n\n",
      "last_modified": "2025-05-07T16:47:00-07:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
