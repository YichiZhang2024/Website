{
  "articles": [
    {
      "path": "about.html",
      "title": "Yichi Zhang 张亦弛",
      "description": "A little bit about myself ",
      "author": [],
      "contents": "\n\n\nHi, I am a Ph.D. candidate in the Quantitative Methods and Computational Psychology program at the University of Southern California (USC). I am from Shandong, China and my native language is mandarin. I pursued my bachelor degree at Dickinson College with double major in Mathematics and Psychology. My interest in using statistical tools to understand human behaviors led me to pursue a Phd degree in quantitative psychology.\nMy current research interests are measurement bias in survey/questionnaires and robust methods for psychological research. I am co-advised by Dr. Hok Chio(Mark) Lai and Dr. Rand Wilcox. I am interested in exploring research questions related to fair and accurate psychological measurement using psychometric methods and statistical models. Check out the Research page for specific projects and Publication page for my publications. CV can be found here.\n\n\n\n",
      "last_modified": "2023-09-20T16:11:53-07:00"
    },
    {
      "path": "index.html",
      "title": "Blog",
      "description": "Welcome to the website. I hope you enjoy it!\n",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2023-09-20T16:11:54-07:00"
    },
    {
      "path": "publications.html",
      "title": "Publications",
      "description": "My recent publications and conference presentations. ",
      "author": [],
      "contents": "\nPeer-Reviewed Articles\n2023\n\nOzturk, E. D., Y. Zhang, M. H. C. Lai, et al. (2023).\n“Measurement Invariance of the Neurobehavioral Symptom\nInventory in Male and Female Million Veteran Program Enrollees\nCompleting the Comprehensive Traumatic Brain Injury\nEvaluation”. In: Assessment. DOI:\n10.1177/10731911231198214.\nZhang, Y., M. H. C. Lai, and G. J. Palardy (2023). “A Bayesian\nregion of measurement equivalence (ROME) approach for\nestablishing measurement invariance.” In: Psychological\nMethods 28.4, pp. 993-1004. DOI:\n10.1037/met0000455.\nZhang, Y., W. W. Tse, and M. H. C. Lai (2023). “Bootstrap\nmethods for multilevel data when asymptotic distributional\nassumptions are not tenable”. Dependent Data in Social\nSciences Research: Forms, Issues, and Methods of\nAnalysis.(Under review).\n\n2022\n\nLai, M. H. C. and Y. Zhang (2022). “Classification Accuracy of\nMultidimensional Tests: Quantifying the Impact of\nNoninvariance”. In: Structural Equation Modeling: A\nMultidisciplinary Journal 29.4, pp. 620-629. DOI:\n10.1080/10705511.2021.1977936.\nTse, W. W., M. H. C. Lai, and Y. Zhang (2022). “Does strict\ninvariance matter? valid group mean comparisons with\nordered-categorical items”. (Accepted on 09/18/2023 by\nBehavior Research Methods).\nZhang, Y. and M. H. C. Lai (2022). “Evaluating Standard Error\nEstimators for Multilevel Models on Small Samples With\nHeteroscedasticity and Unbalanced Cluster Sizes”. (Revision;\nSubmitted to Behavior Research Methods).\n\n2021\n\nLai, M. H. C., Y. Zhang, and J. Feng (2021). “Adjusting for\nmeasurement error in cluster means in multilevel modeling: Two\nnumerically stable alternatives to latent-mean centering.”\n(Second round revision submitted; Multivariate Behavioral\nResearch).\n\nConference Presentations\n2023\nZhang, Y. & Lai, M. H. C. (2023, July 25-28). Alignment with Bayesian Region of Measurement Equivalence (ABROME) Approach for Multiple Groups Comparisons [Oral Presentation]. Annual Meeting of the Psychometric Society (IMPS), College Park, Maryland.\nZhang, Y., Kim, Y., & Zheng, X. (2023, April 12-15). Investigating Measurement Invariance in NAEP Student Questionnaire Index Items [Oral Presentation]. The National Council on Measurement in Education (NCME), Chicago, Illinois.\nLai, M. H. C., Zhang, Y., & Feng J.(2023, April 13-16). An empirical Bayes cluster-mean approach to correct for sampling error in between-cluster effects [Poster session]. American Educational Research Association Annual Meeting (AERA), Chicago, Illinois.\n2022\nZhang, Y. & Lai, M. H. C. (2022, July 11-15). Bayesian Region of Measurement Equivalence Approach with Alignment [Oral Presentation]. Annual Meeting of the Psychometric Society (IMPS), Bologna, Italy.\nZhang, Y. & Lai, M. H. C. (2022, July 11-15). Inferences with Multilevel Model: What if You have Small, Unbalanced, Heteroscedastic Samples? [Poster Presentation]. Annual Meeting of the Psychometric Society (IMPS), Bologna, Italy.\nZhang, Y. & Lai, M. H. C. (2022, April 21-26). Evaluating Standard Error Estimators for Multilevel Models on Small Samples With Heteroscedasticity and Unbalanced Cluster Sizes. American Educational Research Association Annual Meeting, San Diego, Ca, United States.\n2021\nZhang, Y. & Lai, M. H. C. (2021, August 12-14). Classification accuracy of multidimensional tests: Quantifying the impact of noninvariance [Poster Session]. American Psychological Association Annual Convention (APA), Online.\nZhang, Y. & Lai, M. H. C. (2021, July 20-23). Classification accuracy of multidimensional tests: Quantifying the impact of noninvariance [Oral Presentation]. Annual Meeting of the Psychometric Society (IMPS), College Park, MD, United States.\nZhang, R., Zhang, Y., & Lalonde, R. (2021, July 27-31). Examining multiculturalism-creativity link from the perspective of challenge and threat appraisals [Oral Presentation]. International Association of Cross-Cultural Psychology (IACCP), online.\n2020\nZhang, Y. & Lai, M. H. C. (2020, July 14-17). A Bayesian Region of Measurement Equivalence (ROME) Approach for Establishing Measurement Invariance [Poster Session]. Annual Meeting of the Psychometric Society (IMPS), College Park, MD, United States. Schedule\nZhang, Y. & Lai, M. H. C. (2020, June 2-3). A Bayesian Region of Measurement Equivalence (ROME) Approach for Establishing Measurement Invariance [Poster Session]. Modern Modeling Methods Conference (MMM), Storrs, CT, United States. (Conference canceled)\n\n\n\n",
      "last_modified": "2023-09-20T16:11:57-07:00"
    },
    {
      "path": "research.html",
      "title": "Research",
      "author": [],
      "contents": "\nCurrent Projects\nBayesian Region of Measurement Equivalence (ROME) Approach for Establishing Measurement Invariance\nPsychological scales are widely used when making decisions in personnel selections and college admissions. However, as the current generation has become more racially and ethnically diverse, people might react differently to scale items due to their diverse backgrounds and experiences. Thus, some scale items may contain systematic bias that leads to a higher score for some subgroups. While there is abundant research focusing on detecting such bias, most of them rely on the Null hypothesis testing (NHST) framework, with little attention to the degree of bias and the practical impact of bias.\nI proposed a Bayesian region of measurement equivalence (ROME) method for establishing measurement invariance, which allows researchers to quantify the degree of item bias on total scale scores and decide whether the group difference caused by biased indicators is negligible. This method allows researchers to interpret the noninvariance from a practical decision-making perspective. I applied this method on the National Assessment of Educational Progress data during my internship at the American Institutes for Research in 2022.\nEvaluating Standard Error Estimators for Multilevel Models on Small Samples With Heteroscedasticity and Unbalanced Cluster Sizes\nMultilevel modeling (MLM) is commonly used in psychological research to model clustered data. However, data in applied research usually come from small samples and have heteroscedastic variances and unbalanced cluster sizes, which violates one of the essential assumptions of MLM - homogeneity of variance. While the fixed-effect estimates produced by the maximum likelihood method remain unbiased, the standard errors for the fixed effects are mis-estimated, resulting in inaccurate inferences and inflated or deflated Type I error rates. Small-sample corrections, such as the Kenward-Roger (KR) adjustment and the adjusted cluster-robust standard errors (CR-SEs), have been proposed in literature.\nMy research compares KR with random slope (RS) models and the adjusted CR-SEs with Ordinary Least Squares (OLS), random intercept (RI) and RS models to analyze small, heteroscedastic, clustered data using a Monte Carlo simulation. I illustrated the use of these two small sample corrections on empirical data and provide guidelines for applied researchers.\nEvaluating Subgroup Analysis Indices and Guidelines for Automated Scoring Algorithm\nAutomated Essay Scoring (AES) is an application of artificial intelligence that predicts the scores humans would assign to essays based on the content and features of those essays. Williamson, Xi, and Breyer (2012) offered guidelines for evaluating an AES system, and those guidelines recommend that stakeholders evaluate fairness by examining scores assigned to demographic subgroups with respect to distribution shifts and levels of agreement between AES and human scores.\nThis study evaluates the performance of AES agreement indices used for subgroup analyses in terms of these flag rates, and we do so in the context of a simulation in which we vary sample size, distribution shape, and the number of score points when data are generated to produce predetermined levels of agreement or mean score offset between human scores and AES.\n\n\n\n",
      "last_modified": "2023-09-20T16:11:57-07:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
