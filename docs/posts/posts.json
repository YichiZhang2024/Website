[
  {
    "path": "posts/2021-12-23-juliamlm/",
    "title": "Multilevel Models in Julia ",
    "description": {},
    "author": [
      {
        "name": "Yichi Zhang",
        "url": {}
      }
    ],
    "date": "2021-12-23",
    "categories": [],
    "contents": "\nToday We will have a fun session doing regression and multilevel models(MLMs) using Julia. First, we will have a small exercise fitting a linear regression model to a sample dataset, and then we will fit the MLM to the same dataset and compare the difference in results. Please note the examples come from Dr.Mark Lai’s multilevel modeling course, which can be assessed at https://quantscience.rbind.io/courses/psyc575/homework-3/. Can’t wait? Let’s get started!\nSet up\nInstall Packages\nIn Julia, we can add package by typing “]” in the Terminal to enter the Pkg mode and exit using Ctrl + C. Or, we can use [Pkg] package and add multiple packages at once.\n#= uncomment below to install packages\nusing Pkg\nPkg.add([\"Pipe\", \"DataFrames\", \"StatFiles\", \"GLM\",\"MixedModels\",\"Plots\", \"StatsPlots\"])\n=#\nUsing Packages\nJust like we use library to load packages in R, we use using in Julia. It’s better to have a separate session in the beginning of the file to load all packages you will need in the analysis. Otherwise it will take a long time to precompile each time.\nPipe for pipe operator. DataFrames for working with dataframes. StatFiles for reading datasets from Stata, SPSS and SAS. If you have a txt file, can try CSV.read function from package CSV. GLM for linear regression. MixedModels for working with multilevel models. Similar to lme4. Plots for making graphs. StatsPlots for making graphs.\nusing Pipe, DataFrames, StatFiles, GLM, MixedModels, Plots, StatsPlots, Statistics\nDataset\nToday we are going to use the dataset from the World Value Survey-1990-93 data (World Values Study Group, 1994). There are five variables in the data set:\nCountryID: CountryIDcountry: Country’s namegm_GNP: Grand-mean centered Gross National Productincome: Income level(0-least income to 9-most income)happy:Feel happy(1-not happy to 4-very happy)\nThis data set and set of practice problem come from Mark’s Multilvel Modeling class, so thanks Mark!\ndata_happy = DataFrame(load(\"happy_combined.sav\"))\n#= Or we can use pipe operator\ndata_happy = load(\"happy_combined.sav\") |> DataFrame\n=#\ndata_happy = dropmissing(data_happy)\nResearch Questions\nAre people with higher individual level income happier? Is the relation similar across countries? How is the result of linear regression different from the result of multilevel models?\nDescriptive Analysis\n## Summary of all variables in the dataset\ndescribe(data_happy)\n## list first five rows of data\nfirst(data_happy,5)\n## list names of all variables\nnames(data_happy)\n## get size of the data set\nsize(data_happy)\nsize(data_happy,1)\nsize(data_happy,2)\n## check a specific column \ndata_happy[:,\"country\"]\nunique(data_happy[:,\"country\"])\n# data_happy[!,:2]; data_happy[:,2] also work for extracting the second columns\n# data_happy[2,:] can extract the second row, data_happy[2:5,:] extract the second to fifth row.\n@df data_happy scatter(\n    :country, \n    :happy,\n    group = :country)\nLinear Regression\n???Exercise Time: We are familiar with our dataset, so let’s do some exercise! Recall Winnie did a great presentation last time on linear regression, so please go ahead to fit a linear regression model and write out the equation.\nHint: Model_name = lm(@formula(DV ~ IV), data_set)\nFitting the linear regression model\nlm1 = lm(@formula(happy~ gm_GNP),data_happy)\n# extract coefficients\ncoef(lm1)\n# extract standard errors\nstderror(lm1)\n# extract variance covariance matrix\nvcov(lm1)\n# obtain R^2\nr2(lm1)\n# get the deviance \ndeviance(lm1)\nThis model explains 4.72% of variance in happy. Note the standard error estimates for gm_GNP is 0.010, t =17.13, 95% CI [0.155 0.195], p < 0.0001.\nEquations\n\\text{happy} = 2.992 + 0.175 \\text{gm_GNP}\nMultilevel Modeling\nRandom Intercept Model\nWe first fit a random intercept model and calculate the intraclass correlation. Recall\nEquations\nLevel 1:\n\\text{happy}_{ij} = \\beta_{0j} + e_{ij}\nLevel 2:\n\\beta_{0j} = \\gamma_{00} + u_{0j}\n\\text{ICC} = \\frac{\\tau_0^2}{\\tau_0^2 + \\sigma^2}\n## Fitting MLMs\nmm1 = fit(LinearMixedModel, @formula(happy ~ (1|country)), data_happy)\n## Create a vector and store the model fit statistics\nmodel_fit= Vector{Float64}()\npush!(model_fit,aic(mm1))\n??? Exercise Time: What is the value of ICC?\nThe first part of the result prints out estimation method and the model fit statistics, such as AIC,BIC, etc. The second part is the table of estimates of parameters associated with the random effects. The third part is the fixed effects point estimates and standard errors.\nICC = 0.0649/(0.0649 + 0.4842) = 0.118, so there is evidence that people’s happiness level varies across countries. Variability at the country level accounts for 11.8% of the total variability of happiness level.\nDesign effect = 1 +(average cluster size - 1) x ICC. We have 5926 observations and 38 groups, so design effect = 1 + (5926/38 -1) x 0.118 = 19.28.\nAdd Level 2 Predictor\nIt is reasonable to think gm_GNP is a cluster level predictor, so let’s add it to our model.\nEquations\nLevel 1:\n\\text{happy}_{ij} = \\beta_{0j} + e_{ij}\nLevel 2:\n\\beta_{0j} = \\gamma_{00} + \\gamma_{01} \\text{gm_GNP}_{j} + u_{0j}\\\\\n## Fitting MLMs\nmm2 = fit(LinearMixedModel, @formula(happy ~ gm_GNP + (1|country)), data_happy)\npush!(model_fit, aic(mm2))\n# extract log likelihood\nloglikelihood(mm2)\n# extract Akaike's Information Criterion\naic(mm2)\n# extract Bayesian Information Criterion\nbic(mm2)\n# extract degrees of freedom\ndof(mm2)\n# extract coefficient\ncoef(mm2)\n# extract fixed effects\nfixef(mm2)\nvcov(mm2)\nstderror(mm2)\n# extract coefficients table\ncoeftable(mm2)\n# extract variance components\nVarCorr(mm2)\n# return sigma^2\nvarest(mm2)\n# return tau\nVarCorr(mm2).σρ[1][1][1]\n# return elements in the components\ndump(VarCorr(mm1))\n# return sigma\nsdest(mm2)\n# extract random effects\nranef(mm2)\nNote that the result is slightly different from the R output, it’s because the default estimation method in [MixedModels] in Julia is maximum likelihood estimation, whereas the default estimation method for [lme4] in R is Restricted maximum likelihood method.\nThis time standard error estimates for gm_GNP is 0.0337, z = 5.33, p < 0.0001. The SE for OLS is smaller than the SE for MLM, indicating OLS underestimates the SE.\nAdd Level 1 predictor\nBecause relationships at one level are not necessarily the same at the other level, we need to be careful adding the level 1 predictor. Two approaches to decompose the impact of level 1 predictor on outcome variables: cluter mean centering + cluster mean and the raw predictor + clutster mean. Here we will use the first approach.\nEquations\nLevel 1:\n\\text{happy}_{ij} = \\beta_{0j} + \\beta_{1j} \\text{income_cmc}_{ij} + e_{ij}\nLevel 2:\n\\beta_{0j} = \\gamma_{00} + \\gamma_{01} \\text{income_mean}_{j} + u_{0j}\\\\\n\\beta_{1j} = \\gamma_{10} + u_{1j} \n## Cluster mean centering\ndata_happy2 = @pipe data_happy |> \n            groupby(_,:country) |> # group by country\n            transform(_, :income => mean => :income_mean)|> # add a new variable income_mean\n            transform(_, [:income, :income_mean]=> ByRow(-) => :income_cmc) # add a new variable the centered variable\n\n## Fitting MLMs\nmm3 = fit(LinearMixedModel, @formula(happy ~ income_cmc + income_mean + (income_cmc|country)), data_happy2)\npush!(model_fit, aic(mm3))\nBoth income_cmc and income are significant predictors of happiness level. For a person from a country with income_mean = 0 and this person has average country level income, the predicted happiness level is 2.66, SE = 0.16. The average within country slope is 0.047(SE = 0.008), meaning a one unit increase in income_cmc is associated with a 0.047 unit increase in happiness. This slope varies across countries, with a standard deviation of 0.38. The average between country level slope is 0.08, SE = 0.04, suggesting a one unit increase in income_mean is associated with a 0.08 unit increase in the average happiness level.\nCross level Interaction\nIs the relation between happy and income moderated by gm_GNP? We can answer this question by adding gm_GNP to the above model.\nmm4 = fit(LinearMixedModel, @formula(happy ~ income_cmc * gm_GNP + income_mean + (income_cmc|country)), data_happy2)\npush!(model_fit, aic(mm4))\nAfter adding gm_GNP, the impact of income_mean on happy is not significant.\nEquations\nLevel 1:\n\\text{happy}_{ij} = \\beta_{0j} + \\beta_{1j} \\text{income_cmc}_{ij} + e_{ij}\nLevel 2:\n\\beta_{0j} = \\gamma_{00} + \\gamma_{01} \\text{income_mean}_{j} + \\gamma_{02} \\text{gm_GNP}_{j} + u_{0j}\\\\\n\\beta_{1j} = \\gamma_{10} + \\gamma_{11} \\text{gm_GNP}_{j} + u_{1j} \n??? Exercise time: 1. Which model fits data better?\n# print out the AIC\nprint(model_fit)\n# Likelihood Ratio Test\nMixedModels.likelihoodratiotest(mm1,mm2,mm3,mm4)\nConclusion\nThe results of [MixedModels] in Julia and [lme4] are slightly different due to the estimation methods that are used. The cross-level interaction model fits best to our data, suggesting the country level GNP gm_GNP moderated the relation between individual’s happiness level happy and income income.\nResources\nMark’s MLM class and HW\nDataFrames package https://dataframes.juliadata.org/v0.14.0/index.html\nMixedModels Package (similar to lme4 in R) documentation https://juliastats.org/MixedModels.jl/v1.0/index.html\nMaybe useful for plotting week:\nGadfly Package (similar to ggplot in R) documentation https://juliastats.org/MixedModels.jl/v1.0/index.html\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-23T14:38:01-08:00",
    "input_file": {}
  }
]
